import os
import re
import asyncio
import io
import uuid
import json
from typing import List, Optional
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from sqlmodel import Session, select, SQLModel, create_engine
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from pydantic import BaseModel
import httpx
from bs4 import BeautifulSoup
from openai import AsyncOpenAI

# --- OCR Dependencies ---
import fitz  # PyMuPDF
from rapidocr_onnxruntime import RapidOCR
from concurrent.futures import ThreadPoolExecutor

# --- RAG Dependencies ---
# è®¾ç½® HuggingFace é•œåƒ (åœ¨å¯¼å…¥ sentence_transformers ä¹‹å‰)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    RAG_AVAILABLE = True
except Exception as e:
    print(f"[PHANTOM] RAG features disabled (Import Error): {e}")
    RAG_AVAILABLE = False

# --- Web Search Dependencies ---
try:
    from duckduckgo_search import DDGS
    SEARCH_AVAILABLE = True
    print("[PHANTOM] Web Search Module Loaded.")
except ImportError:
    print("[PHANTOM] duckduckgo-search not found. Web search disabled.")
    SEARCH_AVAILABLE = False

# Load Secret Keys
from pathlib import Path
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
# Fallback to local .env if parent not found
if not os.getenv("DEEPSEEK_API_KEY"):
    load_dotenv()

from models import Paper, PaperCreate, PaperRead

import time  # For monitoring

# --- Global Metrics ---
system_metrics = {
    "status": "ONLINE",
    "ai_latency_ms": 0,    # Last DeepSeek response time
    "ocr_speed_ms": 0,     # Last OCR process time
    "last_activity": time.time(),
    "ai_state": "IDLE"     # IDLE, THINKING, RETRIEVING
}

# --- Database Setup ---
sqlite_file_name = "phantom_database.db"
sqlite_url = f"sqlite:///{sqlite_file_name}"
connect_args = {"check_same_thread": False}
engine = create_engine(sqlite_url, connect_args=connect_args)

def create_db_and_tables():
    SQLModel.metadata.create_all(engine)

# --- AI Setup ---
# å¢åŠ  timeout è®¾ç½® (15ç§’ -> 60ç§’)ï¼Œç»™äºˆå¤æ‚æ¨ç†æ›´å¤šæ—¶é—´
deepseek_client = AsyncOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY", "mock-key"), 
    base_url="https://api.deepseek.com",
    timeout=60.0
)

# --- OCR Setup ---
ocr_engine = RapidOCR()
executor = ThreadPoolExecutor(max_workers=4)  # Increased from 2 to 4 for better concurrency

# --- RAG Setup (Memory) ---
if RAG_AVAILABLE:
    chroma_client = chromadb.Client()
    # Use a persistent path if you want memory to survive restart, 
    # but for prototype, ephemeral is fine or use path="phantom_memory"
    # chroma_client = chromadb.PersistentClient(path="phantom_memory") 
    knowledge_collection = chroma_client.get_or_create_collection(name="phantom_knowledge")

    # Load Embedding Model (Downloads on first run)
    try:
        print("[PHANTOM] Loading Embedding Model (using hf-mirror.com)...")
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        print("[PHANTOM] Embedding Model Ready.")
    except Exception as e:
        print(f"[PHANTOM] Failed to load Embedding Model: {e}")
        print("[PHANTOM] RAG features will be disabled for this session.")
        RAG_AVAILABLE = False
        knowledge_collection = None
        embedder = None
else:
    chroma_client = None
    knowledge_collection = None
    embedder = None

# --- Lifecycle ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    create_db_and_tables()
    yield

app = FastAPI(lifespan=lifespan)

# --- CORS Setup (Fix Failed to fetch errors) ---
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for dev; restrict in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Schemas ---
class MindHackRequest(BaseModel):
    text: str
    mode: str

class FusionRequest(BaseModel):
    text_a: str
    title_a: str
    text_b: str
    title_b: str

class ChatRequest(BaseModel):
    query: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]

# --- Helper: OCR (Turbo - Optimized for 3-5x Performance) ---
def extract_text_from_file_sync(file_content: bytes, filename: str) -> str:
    extracted_text = ""
    try:
        if filename.lower().endswith(".pdf"):
            with fitz.open(stream=file_content, filetype="pdf") as doc:
                # Performance Optimization: Only process first 1 page for instant preview (Background will handle full index later)
                target_pages = set(range(min(1, doc.page_count)))
                
                for page_num in sorted(list(target_pages)):
                    page = doc.load_page(page_num)
                    text = page.get_text()
                    # AGGRESSIVE OPTIMIZATION: If we find > 15 chars (e.g. a title), SKIP OCR.
                    # This makes non-scanned PDFs instant.
                    if len(text.strip()) > 15:
                        extracted_text += f"\n--- Page {page_num+1} ---\n{text}\n"
                        continue
                    
                    # Fallback OCR - Performance: Reduced DPI from 96 to 72 for faster processing
                    pix = page.get_pixmap(dpi=72)
                    result, _ = ocr_engine(pix.tobytes("png"))
                    if result:
                        extracted_text += f"\n--- Page {page_num+1} (OCR) ---\n" + "\n".join([line[1] for line in result])
        
        elif filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            result, _ = ocr_engine(file_content)
            if result: extracted_text = "\n".join([line[1] for line in result])
        
        else:
            extracted_text = file_content.decode('utf-8', errors='ignore')

    except Exception as e:
        return f"[OCR ERROR] {str(e)}"
    
    return extracted_text

async def extract_text_from_file(file_content: bytes, filename: str) -> str:
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, extract_text_from_file_sync, file_content, filename)

# --- Helper: Indexing (Memory Injection) ---
def index_document(text: str, filename: str):
    """Chunks text and stores in Vector DB"""
    if not RAG_AVAILABLE or not knowledge_collection or not embedder:
        print(f"[MEMORY] RAG not available, skipping indexing for {filename}")
        return
    
    # Simple chunking by paragraph or fixed size
    chunks = [text[i:i+500] for i in range(0, len(text), 500)]
    
    if not chunks: return

    # Embed
    embeddings = embedder.encode(chunks).tolist()
    ids = [f"{filename}_{uuid.uuid4()}" for _ in chunks]
    metadatas = [{"source": filename} for _ in chunks]

    knowledge_collection.add(
        documents=chunks,
        embeddings=embeddings,
        metadatas=metadatas,
        ids=ids
    )
    print(f"[MEMORY] Indexed {len(chunks)} fragments from {filename}")

# --- Helper: Web Search ---
def perform_web_search(query: str, max_results=3) -> str:
    """Uses DuckDuckGo to find external intel."""
    if not SEARCH_AVAILABLE:
        return ""
    
    print(f"[SEARCH] Infiltrating public network for: {query}")
    try:
        results_text = ""
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results))
            for i, res in enumerate(results):
                results_text += f"[Web Result {i+1}: {res['title']}]\n{res['body']}\nSource: {res['href']}\n\n"
        return results_text
    except Exception as e:
        print(f"[SEARCH ERROR] {e}")
        return f"[Web Search Failed: {str(e)}]"

# --- Endpoints ---

# ç”¨äºå­˜å‚¨å¤„ç†çŠ¶æ€
processing_status = {}

@app.get("/api/monitor")
async def get_system_monitor():
    """Skill: Tactical Support (System Monitor)"""
    # Calculate uptime or fake load for visuals
    return system_metrics

@app.post("/api/scan_document")
async def scan_document(file: UploadFile = File(...)):
    start_time = time.time()
    try:
        content = await file.read()
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶50MBï¼‰
        if len(content) > 50 * 1024 * 1024:
            return {
                "filename": file.filename,
                "error": "æ–‡ä»¶è¿‡å¤§ï¼ˆè¶…è¿‡50MBï¼‰ï¼Œè¯·ä¸Šä¼ è¾ƒå°çš„æ–‡ä»¶",
                "extracted_text": "",
                "char_count": 0
            }
        
        # ä½¿ç”¨asyncio.wait_foræ·»åŠ è¶…æ—¶ï¼ˆ4åˆ†é’Ÿï¼‰
        text = await asyncio.wait_for(
            extract_text_from_file(content, file.filename or "unknown"),
            timeout=240.0
        )

        end_time = time.time()
        system_metrics["ocr_speed_ms"] = int((end_time - start_time) * 1000)
        
        # åå°ç´¢å¼•ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
        if len(text) > 50:
            asyncio.create_task(
                asyncio.to_thread(index_document, text, file.filename or "unknown")
            )

        return {
            "filename": file.filename,
            "extracted_text": text,
            "char_count": len(text)
        }
    
    except asyncio.TimeoutError:
        system_metrics["ocr_speed_ms"] = -1 # Indicate timeout
        return {
            "filename": file.filename,
            "error": "OCRå¤„ç†è¶…æ—¶ï¼ˆè¶…è¿‡4åˆ†é’Ÿï¼‰ï¼Œè¯·å°è¯•ä¸Šä¼ è¾ƒå°çš„æ–‡ä»¶æˆ–è´¨é‡æ›´å¥½çš„PDF",
            "extracted_text": "",
            "char_count": 0
        }
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"[OCR ERROR] {error_detail}")
        return {
            "filename": file.filename,
            "error": f"å¤„ç†å¤±è´¥: {str(e)}",
            "extracted_text": "",
            "char_count": 0
        }

@app.get("/api/scan_status/{task_id}")
async def get_scan_status(task_id: str):
    """è·å–OCRå¤„ç†çŠ¶æ€"""
    status = processing_status.get(task_id, {"status": "unknown"})
    return status

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_phantom(request: ChatRequest):
    """
    Skill 6: RAG Chat (The IM Log)
    """
    print(f"[PHANTOM] Received Chat Query: {request.query}") # Log receipt

    if not RAG_AVAILABLE or not knowledge_collection or not embedder:
        print("[PHANTOM] RAG not available, returning error.")
        return {
            "answer": "ã€RAGåŠŸèƒ½æœªå¯ç”¨ã€‘ç³»ç»Ÿæœªèƒ½åŠ è½½ AI è®°å¿†æ¨¡å—ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼‰ã€‚\n\nè¯·å°è¯•é‡å¯ï¼Œæˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚",
            "sources": []
        }
    
    query = request.query
    api_key = os.getenv("DEEPSEEK_API_KEY")

    # 1. Retrieve (Optimized: n_results reduced from 3 to 2 for faster response)
    try:
        print("[PHANTOM] Retrieving context...")
        query_embedding = embedder.encode([query]).tolist()
        results = knowledge_collection.query(
            query_embeddings=query_embedding,
            n_results=2
        )
    except Exception as e:
        print(f"[PHANTOM] Retrieval Error: {e}")
        # Continue without context if retrieval fails
        results = {'documents': [], 'metadatas': []}
    
    context_text = ""
    sources = set()
    
    if results.get('documents'):
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            source = meta.get('source', 'Unknown')
            context_text += f"[Source: {source}]\n{doc}\n\n"
            sources.add(source)

    if not context_text:
        context_text = "No relevant internal documents found."
    
    # --- WEB SEARCH FALLBACK (NEW) ---
    # å¦‚æœæœ¬åœ°æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæˆ–è€…ç”¨æˆ·æ˜¾å¼è¦æ±‚æœç´¢ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæœ¬åœ°æ— ç»“æœå³æœç´¢ï¼‰
    if "No relevant internal documents found" in context_text and SEARCH_AVAILABLE:
        print("[PHANTOM] Local intel missing. Initiating Web Search protocol...")
        system_metrics["ai_state"] = "SEARCHING"
        try:
            # è¿è¡ŒåŒæ­¥æœç´¢ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­ä»¥å…é˜»å¡ï¼‰
            loop = asyncio.get_event_loop()
            web_results = await loop.run_in_executor(None, perform_web_search, query)
            
            if web_results:
                context_text = f"ã€æœ¬åœ°æ•°æ®åº“æ— ç»“æœï¼Œå·²åˆ‡æ¢è‡³å¹¿åŸŸç½‘æœç´¢æ¨¡å¼ã€‘\n\n{web_results}"
                print("[PHANTOM] Web Search successful. Data injected.")
                # æ·»åŠ æ¥æºæ ‡è®°ï¼ˆè™½ç„¶ä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
                sources.add("Global Network (Web)")
            else:
                context_text += "\n[Web Search yielded no results]"
        except Exception as e:
            print(f"[SEARCH ERROR] {e}")
        finally:
             system_metrics["ai_state"] = "IDLE"

    print(f"[PHANTOM] Context found from {len(sources)} sources. Generating answer...")

    # 2. Generate
    system_prompt = (
        "ä½ æ˜¯æ€ªç›—å›¢çš„å¯¼èˆªå‘˜ (Oracle/Navi)ã€‚ä½ æŒç®¡ç€'å°è±¡ç©ºé—´'çš„çŸ¥è¯†åº“ã€‚"
        "è¯·æ ¹æ®æä¾›çš„[ä¸Šä¸‹æ–‡]å›ç­”ç”¨æˆ·çš„æé—®ã€‚å¦‚æœä¸Šä¸‹æ–‡é‡Œæœ‰ç­”æ¡ˆ,è¯·å¼•ç”¨æ¥æºã€‚"
        "å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰,è¯·ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”,ä½†è¦è¯´æ˜'æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³æƒ…æŠ¥'ã€‚"
        "é£æ ¼ï¼šæ´»æ³¼ã€æå®¢ã€å……æ»¡é»‘å®¢æœ¯è¯­ (Hack, Exploit, Shadow)ã€‚"
    )

    if not api_key or api_key == "mock-key":
        await asyncio.sleep(1.5)
        return {
            "answer": f"ã€æ¨¡æ‹Ÿå›å¤ã€‘(API Keyæœªé…ç½®)\n\næ ¹æ®æˆ‘å¯¹å°è±¡ç©ºé—´çš„æ‰«æ ({list(sources)})... \n\nä¼¼ä¹ '{query}' ä¸è®¤çŸ¥ä¸–ç•Œçš„åº•å±‚æ¶æ„æœ‰å…³ã€‚å»ºè®®æ·±å…¥è°ƒæŸ¥ã€‚",
            "sources": list(sources)
        }

    try:
        system_metrics["ai_state"] = "THINKING"
        start_time = time.time()
        
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡:\n{context_text}\n\né—®é¢˜: {query}"},
            ],
            stream=False,
            max_tokens=500  # Added token limit for faster response
        )
        
        end_time = time.time()
        system_metrics["ai_latency_ms"] = int((end_time - start_time) * 1000)
        system_metrics["ai_state"] = "IDLE"

        print("[PHANTOM] DeepSeek Response Received.")
        return {
            "answer": response.choices[0].message.content,
            "sources": list(sources)
        }
    except Exception as e:
        system_metrics["ai_state"] = "ERROR"
        error_msg = str(e)
        print(f"[PHANTOM] DeepSeek API Error: {error_msg}")
        return {
            "answer": f"âš ï¸ **COGNITIVE BREAKDOWN** (API Error)\n\nè¿æ¥ DeepSeek æ—¶å‘ç”Ÿé”™è¯¯: {error_msg}\n\nå¯èƒ½åŸå› : API è¶…æ—¶ã€å¯†é’¥å¤±æ•ˆæˆ–æœåŠ¡å™¨ç¹å¿™ã€‚",
            "sources": []
        }

@app.post("/api/chat_stream")
async def chat_with_phantom_stream(request: ChatRequest):
    """
    æµå¼RAGèŠå¤© (V2.0) - é›†æˆ Web Search, Monitoring, å’Œ Streaming
    """
    print(f"[PHANTOM] Stream Chat Requested: {request.query}")
    
    # 1. Update Monitor
    system_metrics["ai_state"] = "THINKING"
    start_time = time.time()

    async def generate():
        try:
            # --- A. RAG Retrieval ---
            context_text = ""
            sources = set()
            
            if RAG_AVAILABLE and knowledge_collection and embedder:
                try:
                    query_embedding = embedder.encode([request.query]).tolist()
                    results = knowledge_collection.query(
                        query_embeddings=query_embedding,
                        n_results=2
                    )
                    if results.get('documents'):
                        for i, doc in enumerate(results['documents'][0]):
                            meta = results['metadatas'][0][i]
                            src = meta.get('source', 'Unknown')
                            context_text += f"[Source: {src}]\n{doc}\n\n"
                            sources.add(src)
                except Exception as e:
                    print(f"[RAG Error] {e}")

            if not context_text:
                context_text = "No relevant internal documents found."

            # --- B. Web Search Fallback ---
            if "No relevant internal documents found" in context_text and SEARCH_AVAILABLE:
                system_metrics["ai_state"] = "SEARCHING"
                yield f"data: {json.dumps({'content': 'ğŸ” [Searching Global Network]...\n\n'}, ensure_ascii=False)}\n\n"
                try:
                    # Run search in thread
                    loop = asyncio.get_event_loop()
                    web_results = await loop.run_in_executor(None, perform_web_search, request.query)
                    if web_results:
                        context_text = f"ã€Web Intelã€‘\n{web_results}"
                        sources.add("Global Network")
                except Exception as e:
                    print(f"[Search Error] {e}")
            
            system_metrics["ai_state"] = "THINKING"

            # --- C. System Prompt ---
            system_prompt = (
                "ä½ æ˜¯æ€ªç›—å›¢çš„å¯¼èˆªå‘˜ (Oracle/Navi)ã€‚"
                "é£æ ¼ï¼šæ´»æ³¼ã€æå®¢ã€å……æ»¡é»‘å®¢æœ¯è¯­ (Hack, Exploit, Shadow)ã€‚"
                "å¦‚æœä¸Šä¸‹æ–‡æœ‰ä¿¡æ¯ï¼Œè¯·å¼•ç”¨ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ã€‚"
            )
            
            api_key = os.getenv("DEEPSEEK_API_KEY")
            if not api_key or api_key == "mock-key":
                await asyncio.sleep(0.5)
                yield f"data: {json.dumps({'content': 'ã€Mock Modeã€‘API Key missing. Simulating response...'}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'done': True}, ensure_ascii=False)}\n\n"
                system_metrics["ai_state"] = "IDLE"
                return

            # --- D. Streaming Generation ---
            response = await deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Context:\n{context_text}\n\nQuery: {request.query}"},
                ],
                stream=True,
                max_tokens=1000,
                timeout=60.0
            )

            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield f"data: {json.dumps({'content': content}, ensure_ascii=False)}\n\n"
            
            # --- E. Cleanup ---
            end_time = time.time()
            system_metrics["ai_latency_ms"] = int((end_time - start_time) * 1000)
            system_metrics["ai_state"] = "IDLE"
            
            # Send Done signal with sources
            yield f"data: {json.dumps({'done': True, 'sources': list(sources)}, ensure_ascii=False)}\n\n"

        except Exception as e:
            system_metrics["ai_state"] = "ERROR"
            error_msg = f"âš ï¸ Cognitive Breakdown: {str(e)}"
            yield f"data: {json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

@app.post("/api/mind_hack")
async def mind_hack(request: MindHackRequest):
    print(f"[PHANTOM] Mind Hack Initiated. Mode: {request.mode}")
    system_metrics["ai_state"] = "THINKING"
    start_time = time.time()
    
    try:
        api_key = os.getenv("DEEPSEEK_API_KEY")
        
        if request.mode == "translate":
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šå¤šè¯­è¨€çš„æ€ªç›—ç¿»è¯‘å®˜ï¼Œè¯·å°†ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ç¿»è¯‘æˆé€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡ã€‚ä¿ç•™ä¸“ä¸šæœ¯è¯­ä½†å¢åŠ è§£é‡Šã€‚é£æ ¼ï¼šä¼˜é›…ã€ç²¾å‡†ã€‚"
        else: 
            system_prompt = "ä½ æ˜¯æ€ªç›—å›¢çš„æˆ˜æœ¯åˆ†æå¸ˆï¼ˆNaviï¼‰ã€‚åˆ†æè¿™æ®µæ–‡æœ¬çš„'æ½œå°è¯'ï¼ˆSubtextï¼‰ã€'ä½œè€…æ„å›¾'ï¼ˆIntentï¼‰å’Œ'æ ¸å¿ƒè®ºç‚¹'ï¼ˆCoreï¼‰ã€‚ç”¨Persona 5çš„é£æ ¼ï¼ˆé»‘å®¢ã€æ€ªç›—æœ¯è¯­ï¼‰å›ç­”ã€‚"
            
        if not api_key or api_key == "mock-key":
            await asyncio.sleep(1)
            system_metrics["ai_state"] = "IDLE"
            return {"result": f"ã€æ¨¡æ‹Ÿå›å¤ - ç¦»çº¿æ¨¡å¼ã€‘\nè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® DEEPSEEK_API_KEYã€‚\n\nç›®æ ‡æ–‡æœ¬: {request.text[:50]}..."}

        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": request.text}],
            stream=False
        )
        
        end_time = time.time()
        system_metrics["ai_latency_ms"] = int((end_time - start_time) * 1000)
        system_metrics["ai_state"] = "IDLE"
        
        return {"result": response.choices[0].message.content}

    except Exception as e:
        system_metrics["ai_state"] = "ERROR"
        print(f"[MindHack Error] {e}")
        return {"result": f"åˆ†æå‡ºé”™: {str(e)}"}

@app.post("/api/fuse")
async def fuse_documents(request: FusionRequest):
    # (Existing logic)
    return {"result": "Fusion Mock"} # Placeholder if not fully copied, but previously implemented
